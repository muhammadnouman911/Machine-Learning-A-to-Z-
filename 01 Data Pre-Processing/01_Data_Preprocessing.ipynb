{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T05:00:34.494961Z",
     "start_time": "2024-07-08T05:00:34.486686Z"
    }
   },
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing the data_set and converting it into dependent and independent variable",
   "id": "99c7604d8b7a9ee2"
  },
  {
   "cell_type": "code",
   "id": "77a24eb829ddffaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T05:00:36.417856Z",
     "start_time": "2024-07-08T05:00:36.390678Z"
    }
   },
   "source": [
    "dataset=pd.read_csv('Data.csv');\n",
    "# print(dataset)\n",
    "# Differentiate data set into dependent and independent variable \n",
    "\n",
    "X=dataset.iloc[:,:-1].values\n",
    "\n",
    "\n",
    "#Matrix of Independent VTribal\n",
    "print(X)\n",
    "\n",
    "Y=dataset.iloc[:,3].values\n",
    "print(Y)\n",
    "# Y?"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Handling Missing Data with the help of MEAN",
   "id": "ffca3d7a54de871e"
  },
  {
   "cell_type": "code",
   "id": "4fd2932a922a69cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T05:00:38.774585Z",
     "start_time": "2024-07-08T05:00:38.759849Z"
    }
   },
   "source": [
    "#Missing Data\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer=SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer=imputer.fit(X[:,1:3])\n",
    "X[:,1:3]=imputer.transform(X[:,1:3])\n",
    "\n",
    "# print(X[:,:1])\n",
    "# X?"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transforming the categorical data into numerical data",
   "id": "a41b6e42c6a914ee"
  },
  {
   "cell_type": "code",
   "id": "a1a5f2f3e2612823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T05:00:41.402742Z",
     "start_time": "2024-07-08T05:00:41.384169Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_x=LabelEncoder()\n",
    "X[:,0]=labelencoder_x.fit_transform(X[:,0])\n",
    "X[:,0]\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 2, 1, 0, 2, 0, 1, 0], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Doing oneHOtEncoding so that our model do not compare the value of our categorial variable as high or low",
   "id": "317f9076570925ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T05:01:14.161792Z",
     "start_time": "2024-07-08T05:01:14.139145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#There is a problem with this encodeing : its number model will think 0>1>2 .... but it not the case categorial variable are not greater or less than each other .... For Solving this problem we are going to use dummy variables\n",
    "#There for we will use 03 dummy varible like truth by using oneHorEncode class\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "#Argument should be categorial data feild\n",
    "\n",
    "\n",
    "# Country column\n",
    "# Assuming X is your dataset\n",
    "# Define the ColumnTransformer with OneHotEncoder for the first column (index 0)\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('Country', OneHotEncoder(), [0])\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Fit and transform the dataset\n",
    "X = np.array(ct.fit_transform(X))\n",
    "# Tranformed our coloumn in truth table\n",
    "\n",
    "# For Y as a dependent variable we will use label_encoder\n"
   ],
   "id": "db9ab4a1d255c9c5",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transforming the dependent categorical variable into numerical variable for model",
   "id": "b09ca41b9eb543e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T05:28:38.302120Z",
     "start_time": "2024-07-08T05:28:38.286645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labelencoder_y=LabelEncoder()\n",
    "Y=labelencoder_x.fit_transform(Y)\n",
    "Y"
   ],
   "id": "e8e17b79e6635ab5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Now we are going to split data set into two parts:\n",
    "## 1. Training Set\n",
    "## 2. Test Set\n",
    "\n",
    "### Performance should be same"
   ],
   "id": "3b5eef9d69391c17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T05:28:44.043420Z",
     "start_time": "2024-07-08T05:28:44.032256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ],
   "id": "594a9bac9a99c854",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Feature Scaling\n",
    "\n",
    " #### Feature scaling is a technique used to normalize the range of independent variables or features of data. In the context of machine learning, it is crucial because many algorithms use distances between data points to inform their models, and features with larger ranges can dominate the calculation of distances, which can lead to incorrect results."
   ],
   "id": "f1e9be477fb8083f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T06:00:41.901578Z",
     "start_time": "2024-07-08T06:00:41.886229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x=StandardScaler()\n",
    "X_train=sc_x.fit_transform(X_train)\n",
    "X_test=sc_x.transform(X_test)\n",
    "# we dont fit it in test set\n",
    "#if we scale dummy variable it will good because all things will be om same scale but it will be difficult for us to know which data belong to which country\n",
    "# we don't need to apply feature scaling because it is classification problem with only 0 or 1 , but for regression when dependent variable take huge values, we will do feature scaling"
   ],
   "id": "c527dbd0ea259140",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c5b80569932e6c03"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
